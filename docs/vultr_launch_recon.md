# Vultr Training Launcher â€” Recon Notes

Date: 2025-05-09  
Prepared by: Codex automation agent

## Goals

Lay the groundwork for a Vultr-based training launcher that mirrors the existing Vast.ai workflow. These notes summarize the current Vast launcher features, catalogue Vultr API surface area we must cover, and propose a shared abstraction we can reuse across cloud providers.

## Vast Launcher Feature Inventory

Reference: `scripts/vast_launch.py`

| Capability | Details / Command surface |
|------------|---------------------------|
| **Subcommands** | `setup-env`, `search`, `launch` |
| **Env bootstrap** | Reads `.env`, pushes B2 + WandB credentials via `vastai create env-var` |
| **Instance discovery** | `vastai search offers` with user-provided filters |
| **Launch flow** | Either `vastai create instance <offer>` or `vastai launch instance` with GPU/region filters |
| **On-start script generation** | Clones repo, installs deps, hydrates datasets from Backblaze (`rclone`), optionally precomputes latent cache, runs `scripts/run_fast_to_sota.py`, optional auto-shutdown |
| **Config inlining** | Embeds small YAML configs directly into on-start script for reproducibility |
| **Retries** | Optional retry loop with sleep between attempts |
| **Tagging** | Adds `environment=vast` and run-specific metadata to WandB invocation |

## Vultr API Surface (Minimum Required)

All requests require `Authorization: Bearer $VULTR_KEY` and originate from IPs allow-listed in the Vultr dashboard.

| Task | Endpoint | Notes |
|------|----------|-------|
| Discover GPU plans | `GET /v2/plans?type=gpu` | Returns `plans` with `id`, `locations`, `vcpu_count`, `gpu_count`, `monthly_cost`, `memory`, `disk` |
| Region availability | `GET /v2/regions?availability=gpu` | Filter for regions close to Backblaze (priority: `sjc1`, `sea1`, `lax1`) |
| Create block storage | `POST /v2/blocks` | Body: `{ "region": "...", "size_gb": 1024, "label": "upt-cache" }` |
| Attach block storage | `POST /v2/blocks/{block-id}/attach` | Body includes `instance_id` |
| List block storage status | `GET /v2/blocks/{block-id}` | Poll `status == "active"` before attaching |
| Create instance | `POST /v2/instances` | Fields: `plan`, `region`, `os_id` (Ubuntu 22.04 = 215), `label`, optional `user_data` |
| List instance status | `GET /v2/instances/{id}` | Poll `status == "active"` and `power_status == "running"` |
| Delete instance | `DELETE /v2/instances/{id}` | Graceful teardown |
| Detach/Delete block | `POST /v2/blocks/{block-id}/detach`, `DELETE /v2/blocks/{block-id}` | Respect `--keep-volume` semantics |

## Proposed Shared Abstraction (`scripts/cloud_launch/common.py`)

Create a lightweight layer that encapsulates provider-specific plumbing. Target interface:

```python
class CloudLaunchError(RuntimeError):
    ...

@dataclass
class LaunchSpec:
    plan_id: str
    region: str
    os_id: int
    label: str
    gpu_ram_gb: int
    vcpus: int
    hourly_cost: float
    extra: dict[str, Any] = field(default_factory=dict)

@dataclass
class VolumeSpec:
    size_gb: int
    label: str
    mount_path: Path
    filesystem: str = "ext4"

@dataclass
class BootstrapConfig:
    repo_url: str
    branch: str
    workdir: Path
    config_path: Path
    stage: str
    run_args: list[str]
    precompute_latent_cache: bool
    wandb_tags: list[str]

class CloudProvider(Protocol):
    name: str

    def list_offers(self) -> Iterable[LaunchSpec]: ...
    def provision_volume(self, spec: VolumeSpec) -> str: ...
    def wait_for_volume(self, volume_id: str, *, timeout: float = 300.0) -> None: ...
    def create_instance(self, spec: LaunchSpec, *, user_data: str | None) -> str: ...
    def attach_volume(self, instance_id: str, volume_id: str) -> None: ...
    def run_bootstrap(self, instance_id: str, script: str) -> None: ...
    def destroy_instance(self, instance_id: str) -> None: ...
    def detach_volume(self, instance_id: str, volume_id: str) -> None: ...
```

Provider-specific implementations (Vast, Vultr) can subclass a thin `BaseCloudProvider` that handles HTTP helpers, retries, logging, and shared on-start templating.

## Next Steps

1. Scaffold `scripts/cloud_launch/common.py` with the dataclasses and protocol signatures above.
2. Implement HTTP helper utilities (authenticated session, rate-limit aware retries).
3. Build `scripts/vultr_launch.py` atop the abstraction, mirroring Vast CLI UX.
4. Update `scripts/run_fast_to_sota.py` (or wrapper) to choose provider via flag.

These notes satisfy Phase 0 recon requirements and anchor future implementation phases.

