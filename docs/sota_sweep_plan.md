# SOTA Sweep Plan - Target: nRMSE < 0.035

**Date:** 2025-10-18
**Baseline:** `rerun_txxoc8a8` with nRMSE ~0.09
**Goal:** Achieve nRMSE < 0.035 with physics integrity maintained

---

## Strategy Overview

Following the **Fast-to-SOTA** and **Parallel Runs** playbooks, we execute a multi-round parallel sweep focusing on high-ROI hyperparameters. Each round learns from the previous, promoting only the best candidates to full evaluation.

### Guiding Principles

1. **Orthogonalize:** Test independent axes (optimizer vs. capacity vs. physics)
2. **Fix compute:** Keep batch tokens and evaluation consistent
3. **Low-cardinality first:** 3-5 runs per round to learn fast
4. **Promote quickly:** Top 1-2 from small-eval ‚Üí full-eval ‚Üí champion

---

## Round A: Optimizer & Schedule (3 runs)

**Hypothesis:** The baseline uses suboptimal LR/warmup/EMA; better optimization will yield 20-40% nRMSE reduction.

| Config | LR | Warmup | EMA | Epochs | Tags |
|--------|-------|--------|----------|--------|------|
| `sweep_round_a_lr2e4_w3.yaml` | 2e-4 | 3% | 0.9995 | 20+8+8 | Conservative LR, early warmup |
| `sweep_round_a_lr3e4_w5.yaml` | 3e-4 | 5% | 0.9995 | 20+8+8 | Balanced mid-range |
| `sweep_round_a_lr45e4_w5.yaml` | 4.5e-4 | 5% | 0.9999 | 20+8+8 | Aggressive LR, high EMA |

**Key Changes from Baseline:**
- Changed betas from `[0.9, 0.999]` ‚Üí `[0.9, 0.95]` (more stable, per playbook)
- Extended epochs: operator 15‚Üí20, diff_residual 5‚Üí8, consistency 6‚Üí8
- Added explicit warmup_steps_ratio (baseline missing)
- Varied EMA decay (0.9995 vs 0.9999)

**Expected Outcome:** One config improves nRMSE by ‚â•0.01, passes physics gates ‚Üí promote to Round B.

---

## Round B: Capacity Scaling (2 runs)

**Hypothesis:** Baseline is under-parameterized (16 tokens, 64 hidden); scaling capacity will capture finer dynamics.

| Config | Latent Tokens | Hidden Dim | Depths | Heads | Batch Size | Tags |
|--------|---------------|------------|--------|-------|------------|------|
| `sweep_round_b_capacity_up.yaml` | 32 | 96 | [1,1,1] | 6 | 10 | Width+tokens scale |
| `sweep_round_b_deeper.yaml` | 24 | 80 | [2,2,2] | 5 | 10 | Depth scale |

**Key Changes:**
- **Capacity Up:** 2√ó tokens (16‚Üí32), 1.5√ó hidden (64‚Üí96), scaled heads/groups
- **Deeper:** 2√ó depth ([1,1,1]‚Üí[2,2,2]), moderate width increase
- Adjusted batch size (12‚Üí10) and accum_steps (4‚Üí5) to maintain VRAM budget
- Scaled decoder hidden_dim and num_layers proportionally
- LR scaled linearly with capacity per playbook (3e-4 ‚Üí 4e-4 for capacity_up)

**Expected Outcome:** Capacity increase yields 15-25% nRMSE reduction if baseline was bottlenecked; depth may help long-horizon stability.

---

## Round Aggressive: Combined SOTA Push (1 run)

**Hypothesis:** Combine best optimizer + maximum feasible capacity + extended training ‚Üí target nRMSE < 0.035.

| Config | Latent Dim/Tokens | Hidden | Depths | Epochs | TTC Steps | Tags |
|--------|-------------------|--------|--------|--------|-----------|------|
| `sweep_aggressive_sota.yaml` | 48/48 | 128 | [2,2,2] | 30+12+12 | 2 | Large model, extended |

**Key Changes:**
- **Massive capacity:** dim 32‚Üí48, tokens 16‚Üí48, hidden 64‚Üí128
- **Extended training:** operator 15‚Üí30, diff 5‚Üí12, consistency 6‚Üí12
- **Enhanced TTC:** steps 1‚Üí2, candidates 8‚Üí12, beam 3‚Üí4, tighter threshold (0.35‚Üí0.3)
- **Stronger physics weighting:** gamma 1.0‚Üí1.2, mass weight 1.2‚Üí1.5
- **More frequencies:** Added 16.0 to decoder
- **Optimized regularization:** lambda_spectral 0.05‚Üí0.08, weight_decay 0.01‚Üí0.03
- Batch size reduced to 8 (VRAM), accum_steps increased to 6

**Expected Outcome:** If capacity was the primary bottleneck, this should achieve nRMSE < 0.05, potentially < 0.035 with TTC refinement.

---

## Execution Plan

### Phase 1: Launch Parallel Runs (6 instances total)

1. **Round A (3 instances):** optimizer sweep - smallest VRAM, fastest iteration
2. **Round B (2 instances):** capacity sweep - moderate VRAM
3. **Aggressive (1 instance):** large model - high VRAM, longest runtime

**Compute Allocation:**
- Round A configs: A100 40GB (cheaper, sufficient)
- Round B configs: A100 40GB or 80GB (moderate)
- Aggressive config: A100 80GB (required)

**Vast.ai Search Criteria:**
```bash
vastai search offers 'gpu_ram >= 40 reliability > 0.95 num_gpus=1 disk_space >= 64' --order 'dph_total'
# For aggressive: gpu_ram >= 80
```

### Phase 2: Monitor & Gate (6-12 hours)

- All runs log to W&B under `universal-simulator` project
- Round A group: `sota-sweep-round-a`
- Round B group: `sota-sweep-round-b`
- Aggressive group: `sota-aggressive`

**Small-Eval Gating:**
- Must improve nRMSE by ‚â•0.01 vs baseline (0.09 ‚Üí ‚â§0.08)
- Conservation gap ‚â§1.0√ó baseline
- BC violation ‚â§1.0√ó baseline
- ECE ‚â§1.25√ó baseline

### Phase 3: Promote Winners (12-24 hours)

- Top 1 from Round A ‚Üí full-eval
- Top 1 from Round B ‚Üí full-eval
- Aggressive ‚Üí full-eval (forced, even if small-eval marginal)

**Full-Eval Promotion:**
- nRMSE < 0.08 ‚Üí new baseline
- nRMSE < 0.05 ‚Üí strong candidate
- nRMSE < 0.035 ‚Üí **SOTA achieved**

### Phase 4: Iteration (if needed)

If no config reaches < 0.035:

**Round C: Local BO around best**
- Take top config from A+B
- Vary ¬±20% around winner: LR, grad_clip, TTC threshold, lambda_spectral
- 6-12 trials via W&B sweep

**Round D: Architecture refinement**
- If aggressive model shows promise but not quite there:
  - Extend epochs further (40+15+15)
  - Add stochastic depth 0.1-0.2
  - Tune TTC reward weights more aggressively
  - Try dim 64, tokens 64 (if VRAM allows)

---

## Checkpoints & Artifacts

Each run produces:
- `checkpoints/{operator,diffusion}_ema.pt`
- `artifacts/runs/{run_id}/summary.json`
- `reports/leaderboard.csv` (updated)
- `reports/eval_{run_id}.{json,csv,html}`
- W&B artifacts (config, metrics, plots)

**Leaderboard Columns:**
- `run_id`, `timestamp`, `label`, `nrmse`, `conservation_gap`, `bc_violation`, `ece`, `wall_clock`, `params`, `tags`

---

## Success Criteria

### Minimum Acceptable (Baseline Improvement)
- nRMSE ‚â§ 0.07 (22% reduction)
- Conservation gap ‚â§ baseline
- Wall-clock ‚â§ 2√ó baseline

### Target (Strong SOTA)
- nRMSE ‚â§ 0.05 (44% reduction)
- Conservation gap ‚â§ 0.5√ó baseline
- ECE ‚â§ baseline

### Stretch Goal (Full SOTA)
- **nRMSE < 0.035** (61% reduction)
- Conservation gap < 0.3√ó baseline
- BC violation < 0.5√ó baseline
- ECE ‚â§ baseline
- Rollout horizon@œÅ‚â•0.8 ‚â• 64 steps

---

## Risk Mitigation

### If OOM on Vast.ai
- Reduce batch_size by 1-2
- Increase accum_steps proportionally
- Disable compile temporarily
- Use fp32 for latent_cache_dtype ‚Üí bf16

### If Training Diverges
- Lower LR by 0.7√ó
- Increase warmup to 8%
- Lower grad_clip to 0.5
- Check for NaNs in TTC reward computation

### If Physics Gates Fail
- Increase lambda_spectral (0.05 ‚Üí 0.1)
- Raise TTC mass/energy weights
- Apply post-decode Hodge projection
- Reduce guidance_lambda (1.0 ‚Üí 0.7)

### If Runs Stall on Vast.ai
- SSH check: `vastai ssh <instance_id> 'tail -50 nohup.out'`
- Kill stuck instance: `vastai destroy instance <id>`
- Relaunch with same config + `--resume` flag (if implemented)

---

## Timeline Estimate

| Phase | Duration | Parallel | Notes |
|-------|----------|----------|-------|
| Config creation | 1 hr | - | Done |
| Instance launch | 0.5 hr | - | Automated via vast_launch.py |
| Data download | 0.5 hr | Yes | Parallel across instances |
| Training (Round A) | 6-8 hr | 3√ó | Shortest (small model) |
| Training (Round B) | 8-10 hr | 2√ó | Moderate (medium model) |
| Training (Aggressive) | 12-16 hr | 1√ó | Longest (large model) |
| Small-eval | 0.5 hr | All | Concurrent |
| Full-eval (top 3) | 2-3 hr | 3√ó | Concurrent |
| Analysis & comparison | 1 hr | - | Post-processing |
| **Total (wall-clock)** | **16-20 hr** | **6√ó** | End-to-end |

**Cost Estimate (Vast.ai):**
- Round A (3√ó A100 40GB @ $0.40/hr √ó 8hr) = $9.60
- Round B (2√ó A100 40GB @ $0.40/hr √ó 10hr) = $8.00
- Aggressive (1√ó A100 80GB @ $0.95/hr √ó 16hr) = $15.20
- **Total: ~$33** for full sweep

---

## Next Steps

1. ‚úÖ **Create configs** (completed)
2. üîÑ **Search Vast.ai instances** (next)
3. üîÑ **Launch 6 parallel runs** (next)
4. ‚è≥ **Monitor W&B dashboards** (after launch)
5. ‚è≥ **Apply gates & promote** (after small-eval)
6. ‚è≥ **Iterate if needed** (conditional)

---

## Config Summary Table

| Config | Type | Tokens | Dim | Hidden | Depths | Heads | LR | Warmup | EMA | Epochs | VRAM Est. |
|--------|------|--------|-----|--------|--------|-------|-------|--------|----------|--------|-----------|
| `sweep_round_a_lr2e4_w3` | Opt | 16 | 32 | 64 | [1,1,1] | 4 | 2e-4 | 3% | 0.9995 | 20+8+8 | ~18GB |
| `sweep_round_a_lr3e4_w5` | Opt | 16 | 32 | 64 | [1,1,1] | 4 | 3e-4 | 5% | 0.9995 | 20+8+8 | ~18GB |
| `sweep_round_a_lr45e4_w5` | Opt | 16 | 32 | 64 | [1,1,1] | 4 | 4.5e-4 | 5% | 0.9999 | 20+8+8 | ~18GB |
| `sweep_round_b_capacity_up` | Cap | 32 | 32 | 96 | [1,1,1] | 6 | 4e-4 | 5% | 0.9995 | 20+8+8 | ~28GB |
| `sweep_round_b_deeper` | Cap | 24 | 32 | 80 | [2,2,2] | 5 | 3.5e-4 | 6% | 0.9995 | 22+8+8 | ~24GB |
| `sweep_aggressive_sota` | Agg | 48 | 48 | 128 | [2,2,2] | 8 | 3.5e-4 | 6% | 0.9997 | 30+12+12 | ~55GB |

---

*End of Plan*
