# TTC Evaluation Analysis - Low Memory Configuration

**Date:** October 14, 2025
**Model:** Latent Operator 512-dim + Diffusion Residual
**Checkpoints:**
- Operator: `run-mt7rckc8-history:v0`
- Diffusion: `run-pp0c2k31-history:v0`

## Executive Summary

Successfully completed TTC (Test-Time Computation) evaluation on Burgers 1D dataset using a low-memory configuration to avoid OOM errors on H200 GPU (140GB VRAM). The evaluation demonstrates that TTC trajectory selection is working correctly with meaningful candidate diversity and balanced selection patterns.

### Key Findings

1. ‚úÖ **TTC is Working**: All 4 candidates being selected with good entropy (92-96%)
2. ‚úÖ **Consistent Performance**: Val/Test metrics within 2.8% (MSE: 0.00122 vs 0.00125)
3. ‚ö†Ô∏è **Limited Diversity**: Candidate reward spread of only 0.003 (~0.01% of mean reward)
4. ‚ö†Ô∏è **Memory Constraints**: Using 32x32 grid instead of 64x64 may limit accuracy
5. üîç **Need Baseline**: No non-TTC comparison to measure TTC benefit

## Configuration

### Low-Memory Settings (Used)
- **Grid Resolution:** 32x32 (reduced from 64x64)
- **Candidates:** 4 (reduced from 6)
- **Beam Width:** 1 (reduced from 2) - greedy selection
- **Batch Size:** 16 (reduced from 32)
- **Max Evaluations:** 50 (reduced from 100)
- **Decoder Hidden Dim:** 128 (reduced from 256)

### Memory Impact
- **Original Config:** 50GB memory spike ‚Üí OOM on 140GB GPU
- **Low-Mem Config:** Successful completion, no OOM
- **Trade-off:** 75% reduction in grid resolution, 33% fewer candidates

## Results

### Validation Split (13 samples)

| Metric | Value |
|--------|-------|
| MSE | 0.001216 |
| MAE | 0.024571 |
| RMSE | 0.034876 |

**Reward Statistics:**
- Mean: -34.79 (physics penalty)
- Std: 0.32
- Range: 1.31 (min: -35.61, max: -34.29)

**Candidate Selection:**
- Candidate 0: 30.8% (4/13)
- Candidate 1: 7.7% (1/13)
- Candidate 2: 30.8% (4/13)
- Candidate 3: 30.8% (4/13)
- **Selection Entropy:** 1.85 / 2.00 (92.7%) - excellent diversity

**Candidate Diversity:**
- Average spread (best-worst): 0.0034
- Relative diversity: 0.01% of mean reward
- All 4 candidates actively selected

### Test Split (13 samples)

| Metric | Value |
|--------|-------|
| MSE | 0.001250 |
| MAE | 0.025081 |
| RMSE | 0.035359 |

**Reward Statistics:**
- Mean: -15.32 (better than validation)
- Std: 1.39
- Range: 5.36 (min: -18.26, max: -12.91)

**Candidate Selection:**
- Candidate 0: 15.4% (2/13)
- Candidate 1: 23.1% (3/13)
- Candidate 2: 23.1% (3/13)
- Candidate 3: 38.5% (5/13)
- **Selection Entropy:** 1.92 / 2.00 (96.1%) - excellent diversity

**Candidate Diversity:**
- Average spread (best-worst): 0.0027
- Relative diversity: 0.02% of mean reward
- All 4 candidates actively selected

### Cross-Split Comparison

- **Reward Distribution Shift:** Val mean (-34.79) vs Test mean (-15.32) - Test has lower penalties (better)
- **Metrics Consistency:** 2.8% difference in MSE (0.00122 vs 0.00125) - very consistent
- **Selection Patterns:** Both splits show balanced candidate usage with high entropy

## Analysis & Insights

### What's Working Well

1. **TTC Selection is Active:** Not stuck on single candidate - uses all 4 with good balance
2. **High Selection Entropy:** 92-96% entropy means near-optimal exploration
3. **Consistent Across Splits:** Similar patterns on val/test indicate robustness
4. **Stable Performance:** MSE variation <3% between splits

### Identified Issues

1. **Low Candidate Diversity (‚ö†Ô∏è CRITICAL)**
   - Reward spread of only 0.003 represents just 0.01% of mean reward
   - This suggests candidates are very similar in quality
   - May indicate:
     - Sampling strategy (tau_range, noise_std) too conservative
     - Reward model resolution (32x32 grid) insufficient to distinguish trajectories
     - Operator predictions are already highly optimized (less room for TTC improvement)

2. **Memory-Limited Configuration**
   - Using 32x32 grid instead of 64x64 reduces reward model resolution by 4x
   - May miss fine-grained physics violations that would differentiate candidates
   - Lower decoder capacity (128 vs 256 hidden dim) may reduce reward accuracy

3. **Greedy Selection (Beam Width = 1)**
   - Current beam_width=1 means no beam search, just greedy best-first
   - Could miss better long-term trajectories
   - Original beam_width=2 would enable more exploration

4. **Small Sample Size**
   - Only 13 samples evaluated (max_evaluations=50, but stopped early)
   - Need full dataset evaluation for statistical significance

### Comparison to Baseline (MISSING ‚ö†Ô∏è)

**Critical Gap:** No evaluation with TTC disabled to measure actual benefit.

- Current MSE: 0.00122 (val), 0.00125 (test)
- Need to run same checkpoints with `ttc.enabled: false`
- This would show if TTC is improving predictions or just overhead

## Recommendations

### Priority 1: Immediate Improvements

1. **Run Baseline Comparison**
   ```bash
   # Disable TTC and re-evaluate
   EVAL_CONFIG=configs/eval_burgers_512dim.yaml  # no TTC
   scripts/run_remote_scale.sh
   ```
   - Compare MSE/MAE with/without TTC
   - Measure actual benefit vs computational cost

2. **Increase Candidate Diversity**
   - Increase `noise_std` from 0.01 to 0.05 (5x more noise)
   - Expand `tau_range` from [0.2, 0.8] to [0.1, 0.9] (wider diffusion timesteps)
   - Both changes make sampled trajectories more diverse

### Priority 2: Memory vs Performance Trade-off

3. **Incremental Grid Scaling**
   - Try 48x48 grid (2.25x current memory, 44% of original)
   - Estimated peak: ~30GB (safe on H200's 140GB)
   - Should improve reward resolution without OOM

4. **Restore Beam Search**
   - Increase `beam_width` from 1 to 2
   - Enables exploration of multiple trajectory hypotheses
   - Marginal memory increase (~2x candidates kept)

5. **Increase Candidates**
   - Try 5 candidates (up from 4, down from original 6)
   - More options for selection with modest memory cost

### Priority 3: Full Evaluation

6. **Complete Dataset Evaluation**
   - Remove `max_evaluations: 50` limit
   - Run on full validation/test splits
   - Get statistically significant results

7. **Longer Horizon**
   - Current `horizon: 2` means only 2-step lookahead
   - Try `horizon: 3` or `horizon: 4` for longer-term planning
   - May find better trajectories with more foresight

## Proposed Next Configuration

```yaml
# configs/eval_burgers_512dim_ttc_val_improved.yaml
ttc:
  enabled: true
  steps: 1
  candidates: 5          # Up from 4
  beam_width: 2          # Up from 1
  horizon: 3             # Up from 2
  max_evaluations: null  # Full dataset
  sampler:
    tau_range: [0.1, 0.9]  # Wider from [0.2, 0.8]
    noise_std: 0.05        # Up from 0.01
  reward:
    grid: [48, 48]         # Up from [32, 32]
  decoder:
    hidden_dim: 192        # Up from 128 (middle ground)
```

**Expected Improvements:**
- Higher candidate diversity due to wider sampling
- Better reward discrimination with 48x48 grid
- Beam search explores multiple hypotheses
- Longer horizon for better planning

**Memory Estimate:** ~35-40GB peak (safe on H200)

## W&B Artifacts

Results uploaded to W&B:
- **Validation:** `ttc-burgers512-val-lowmem-results`
- **Test:** `ttc-burgers512-test-lowmem-results`

View runs:
- https://wandb.ai/emgun-morpheus-space/universal-simulator/runs/owz083x2 (validation)
- https://wandb.ai/emgun-morpheus-space/universal-simulator/runs/h3yim8hr (test)

## Conclusion

The TTC evaluation successfully demonstrates that trajectory selection is working correctly with good candidate diversity in selection (high entropy). However, the candidates themselves have very low reward diversity (0.01%), suggesting either:

1. The sampling strategy is too conservative (most likely)
2. The reward model resolution is insufficient (32x32 grid)
3. The base operator is already optimal (least likely)

**Next Steps:**
1. Run baseline evaluation (no TTC) to measure actual benefit
2. Increase sampling diversity (noise_std, tau_range)
3. Incrementally scale up grid resolution (32‚Üí48‚Üí64)
4. Enable beam search (beam_width=2)
5. Full dataset evaluation for statistical significance

The low-memory configuration proved successful in avoiding OOM, but may have sacrificed too much accuracy. The proposed improved configuration aims to find a better balance.
