include: train_burgers_quality_v2.yaml

# 64-dim optimal model with pru2jxc4 training approach
# 
# Based on TTC results showing 32-dim + TTC = 0.09 NRMSE:
# - 64-dim baseline should improve on 32-dim baseline (0.78 NRMSE)
# - 64-dim + TTC should approach SOTA (~0.05 NRMSE target)
# 
# This tests whether 64-dim offers optimal capacity/cost trade-off:
# - 4× more parameters than 32-dim (more capacity)
# - 64× fewer parameters than 512-dim (faster, cheaper)
# - Potential sweet spot for production deployment

data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

latent:
  dim: 64        # 2× 32-dim, 1/8 of 512-dim
  tokens: 32     # Scale proportionally with dim (32-dim had 16)

training:
  time_stride: 2
  batch_size: 10  # Slightly lower than 32-dim due to larger model
  
  # pru2jxc4 settings
  num_workers: 8
  use_parallel_encoding: true
  
  latent_cache_dir: data/latent_cache
  checkpoint_interval: 50
  
  distill_micro_batch: 3
  distill_num_taus: 5
  
  compile: true
  pin_memory: true
  
  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

operator:
  pdet:
    input_dim: 64      # Match latent dim
    hidden_dim: 128    # 2× latent dim
    depths: [1, 1, 1]
    group_size: 16     # Divides 128 evenly, good for attention
    num_heads: 4

diffusion:
  latent_dim: 64
  hidden_dim: 128

stages:
  operator:
    epochs: 15  # Match pru2jxc4 (constant LR is key)
    
    optimizer:
      name: adamw
      lr: 1.0e-3            # CONSTANT LR (pru2jxc4's key insight)
      weight_decay: 0.02
    
    # NO SCHEDULER = constant LR

  diff_residual:
    epochs: 5
    grad_clip: 1.0
    ema_decay: 0.999
    
    optimizer:
      name: adamw
      lr: 3.0e-5
      weight_decay: 0.01
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 5
      eta_min: 3.0e-6

  consistency_distill:
    epochs: 6
    batch_size: 6
    
    optimizer:
      name: adamw
      lr: 2.0e-5
      weight_decay: 0.01
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 6
      eta_min: 2.0e-6

evaluation:
  enabled: true  # Auto-run evaluation after training

ttc:
  enabled: true
  steps: 1
  candidates: 6
  beam_width: 2
  horizon: 2
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 100
  sampler:
    tau_range: [0.2, 0.8]
    noise_std: 0.01
    noise_schedule: [0.02, 0.01]
  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []
    weights:
      mass: 1.0
      energy: 0.1
      penalty_negative: 0.5
    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1
  decoder:
    latent_dim: 64
    query_dim: 2
    hidden_dim: 128
    mlp_hidden_dim: 128
    num_layers: 2
    num_heads: 4
    frequencies: [1.0, 2.0, 4.0]
    output_channels:
      rho: 1
      e: 1

logging:
  wandb:
    enabled: true
    run_name: burgers64-pru2jxc4-unified
    tags: [64dim, pru2jxc4_config, constant_lr, optimal_capacity, unified_pipeline]

# EXPECTED OUTCOMES:
# ─────────────────────────────────────────────────────────────────────────────
#
# Baseline (no TTC):
#   32-dim: NRMSE = 0.78
#   64-dim: NRMSE = 0.3-0.5 (expected improvement)
#   512-dim: NRMSE = 0.02 (target)
#
# With TTC:
#   32-dim + TTC: NRMSE = 0.09
#   64-dim + TTC: NRMSE = 0.04-0.06 (expected)
#   512-dim + TTC: NRMSE = 0.01-0.02 (expected SOTA)
#
# If 64-dim + TTC achieves ~0.05 NRMSE:
#   → Excellent capacity/cost trade-off
#   → Recommended for production deployment
#   → 64× cheaper than 512-dim, competitive performance
#
# ─────────────────────────────────────────────────────────────────────────────
# Training time: ~25-30 min on H200 (between 32-dim and 512-dim)
# Cost: ~$1.00 @ $2.11/hr
#
# Model size comparison:
#   32-dim:  ~500KB checkpoint
#   64-dim:  ~1MB checkpoint (this config)
#   512-dim: ~500MB checkpoint

