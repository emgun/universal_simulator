# 32-dim v2 FIXED: Self-contained config for end-to-end pipeline
#
# All architecture parameters explicitly defined to avoid inheritance issues
# WandB configured for offline mode with proper sync
# Unified training + evaluation pipeline

# NO INCLUDE - self-contained to avoid inheritance issues

data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  shard_limit: null
  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

latent:
  dim: 32
  tokens: 32

training:
  batch_size: 12
  time_stride: 2
  dt: 0.1
  patience: 10
  
  # Data loading
  num_workers: 8
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2
  
  # Caching
  latent_cache_dir: data/latent_cache
  latent_cache_dtype: float32
  checkpoint_interval: 50
  
  # Optimization
  amp: true
  compile: true
  grad_clip: 1.0
  ema_decay: 0.999
  accum_steps: 4
  
  # Distillation
  distill_micro_batch: 3
  distill_num_taus: 5
  
  # Regularization
  lambda_spectral: 0.05
  lambda_relative: 0.0
  
  # Tau distribution
  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

# OPERATOR ARCHITECTURE - Explicitly defined
operator:
  pdet:
    input_dim: 32          # Must match latent.dim
    hidden_dim: 96         # 1.5× capacity upgrade
    depths: [1, 1, 1]
    group_size: 12         # Divides hidden_dim evenly
    num_heads: 6           # More attention heads

# DIFFUSION ARCHITECTURE - Explicitly defined
diffusion:
  latent_dim: 32           # Must match latent.dim
  hidden_dim: 96           # MUST match operator.pdet.hidden_dim
  num_layers: 3            # Default
  time_embed_dim: 128      # Default
  dropout: 0.0             # Default

# TRAINING STAGES
stages:
  operator:
    epochs: 25             # More training than v1 (was 15)
    
    optimizer:
      name: adamw
      lr: 1.0e-3           # Constant LR (pru2jxc4 proven method)
      weight_decay: 0.03
      betas: [0.9, 0.999]
    
    # NO scheduler = constant LR

  diff_residual:
    epochs: 8              # More than v1 (was 5)
    grad_clip: 1.0
    ema_decay: 0.999
    
    optimizer:
      name: adamw
      lr: 5.0e-5
      weight_decay: 0.015
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 3.0e-6

  consistency_distill:
    epochs: 8              # More than v1 (was 6)
    batch_size: 6
    
    optimizer:
      name: adamw
      lr: 3.0e-5
      weight_decay: 0.015
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 2.0e-6

  steady_prior:
    epochs: 0              # Disabled for now

# EVALUATION - Runs after training
evaluation:
  enabled: true
  split: test              # Use test set

# TTC CONFIGURATION
ttc:
  enabled: true
  steps: 1
  candidates: 8            # More than v1 (was 6)
  beam_width: 3            # Wider than v1 (was 2)
  horizon: 2
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 150     # More than v1 (was 100)
  
  sampler:
    tau_range: [0.15, 0.85]
    noise_std: 0.015
    noise_schedule: [0.03, 0.015, 0.005]
  
  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []
    weights:
      mass: 1.2
      energy: 0.15
      penalty_negative: 0.6
    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1
  
  decoder:
    latent_dim: 32         # Must match latent.dim
    query_dim: 2
    hidden_dim: 96         # Must match operator.pdet.hidden_dim
    mlp_hidden_dim: 128
    num_layers: 3
    num_heads: 6
    frequencies: [1.0, 2.0, 4.0, 8.0]
    output_channels:
      rho: 1
      e: 1

# LOGGING - WandB offline mode
logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers32-v2-fixed
    tags: [32dim, v2_fixed, end_to_end, unified_pipeline, self_contained]
    group: 32dim-v2
    job_type: training-eval

# EXPECTED RESULTS:
# ─────────────────────────────────────────────────────────────────────────────
# Training:
#   Operator: ~2-3e-5 loss (25 epochs)
#   Diffusion: ~5e-5 loss (8 epochs) 
#   Consistency: ~6e-7 loss (8 epochs)
#
# Evaluation (automatic):
#   Baseline: ~0.4-0.5 NRMSE (operator only, deterministic)
#   TTC: ~0.06-0.07 NRMSE (with beam search optimization)
#
# Time: ~35-40 min training + 15-20 min eval = ~55-60 min total on H200
# Cost: ~$2.00 @ $2.11/hr
#
# WandB: Logs offline, sync with: wandb sync wandb/offline-run-*
# ─────────────────────────────────────────────────────────────────────────────

