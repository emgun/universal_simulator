# Ablation: 128 tokens (FIXED)
# Purpose: Fixed configuration for UPT Phase 2 with proper inverse loss weights
# Changes from original:
#   - Inverse loss weights: 0.5 → 0.01 (50x reduction)
#   - Inverse loss frequency: 10 → 1 (every iteration)
#   - Warmup epochs: 15 → 5 (faster ramp-up)
#   - Total epochs: 25 → 40 (more time for inverse loss training)
#   - Batch size: 6 → 10 (better gradient stability)
#   - Accum steps: 6 → 4 (maintain reasonable effective batch)
#   - Learning rate: 5e-4 → 1.4e-3 (scaled with sqrt(d_model))

seed: 42
deterministic: false
benchmark: true

data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

latent:
  dim: 128
  tokens: 128

operator:
  pdet:
    input_dim: 128
    hidden_dim: 256
    depths: [3, 3, 2]
    group_size: 32
    num_heads: 8

diffusion:
  latent_dim: 128
  hidden_dim: 256

training:
  batch_size: 10  # Requires 80GB GPU
  time_stride: 2
  dt: 0.1
  patience: 5    # Stop early if no improvement (was 15, too high)

  num_workers: 6
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2

  checkpoint_interval: 25

  amp: true
  compile: true
  grad_clip: 1.0
  ema_decay: 0.999
  accum_steps: 4

  # UPT Inverse Losses (FIXED)
  lambda_inv_enc: 0.01  # Reduced from 0.5 (50x reduction)
  lambda_inv_dec: 0.01  # Reduced from 0.5 (50x reduction)
  use_inverse_losses: true
  inverse_loss_frequency: 1  # Every iteration (was 10)
  inverse_loss_warmup_epochs: 5  # Faster warmup (was implicit 15)
  inverse_loss_max_weight: 0.05  # Explicit cap

  lambda_spectral: 0.05
  lambda_relative: 0.0

stages:
  operator:
    epochs: 40  # Increased from 25 for proper inverse loss training

    optimizer:
      name: adamw
      lr: 1.4e-3  # Increased from 5e-4 (scaled with sqrt(128/32))
      betas: [0.9, 0.999]
      weight_decay: 0.03

  diff_residual:
    epochs: 3  # Reduced from 8 (less overfitting)

  consistency_distill:
    epochs: 3  # Reduced from 8

checkpoint:
  dir: checkpoints

evaluation:
  enabled: true
  split: test

ttc:
  enabled: true
  debug: true  # ✅ ARM FIX: Enable detailed logging
  steps: 3  # Increased from 1 for actual TTC
  candidates: 12  # Medium model: moderate candidates
  beam_width: 4
  horizon: 2  # ✅ ARM FIX: Enable lookahead (was 1)
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 175

  sampler:
    tau_range: [0.05, 0.95]  # ✅ ARM FIX: Wider range for diversity (was [0.15, 0.85])
    noise_std: 0.10  # ✅ ARM FIX: Moderate noise for medium model
    noise_schedule: [0.12, 0.10, 0.06]  # Updated for higher noise

  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []

    weights:
      # ✅ ARM FIX: Disable conservation penalties for dissipative Burgers PDE
      mass: 0.0  # Burgers does NOT conserve mass (was 1.0)
      energy: 0.0  # Burgers does NOT conserve energy (was 1.0)
      penalty_negative: 1.0  # Only penalize unphysical negatives (was 0.5)

    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1

  decoder:
    latent_dim: 128
    query_dim: 2
    hidden_dim: 256
    mlp_hidden_dim: 256
    num_layers: 3
    num_heads: 4
    frequencies: [1.0, 2.0, 4.0, 8.0]
    output_channels:
      rho: 1
      e: 1

logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers-ablation-128tokens-fixed
    tags: [ablation, 128tokens, 128dim, phase2, fixed, inverse-losses-fixed]
    group: upt-ablation-fixed