# Baseline (no TTC) eval for advection1d + darcy2d. Optimized for faster throughput.

seed: 42
deterministic: false
benchmark: true

data:
  task: [advection1d, darcy2d]
  split: train
  root: data/pdebench
  patch_size: 1
  shard_limit: 200          # cap samples per task to finish quickly
  task_sampling:
    strategy: balanced

latent:
  dim: 128
  tokens: 128

operator:
  architecture_type: pdet_stack
  pdet:
    input_dim: 128
    hidden_dim: 384
    depth: 12
    num_heads: 8
    attention_type: standard
    qk_norm: true
    mlp_ratio: 4.0
    drop_path: 0.1
    dropout: 0.0

diffusion:
  latent_dim: 128
  hidden_dim: 384

training:
  num_gpus: 2
  use_fsdp2: false
  batch_size: 8             # bump batch for throughput (effective batch 16 on 2 GPUs)
  accum_steps: 1            # no need to accumulate with larger batch
  time_stride: 2
  dt: 0.1
  patience: 5
  num_workers: 8
  use_parallel_encoding: false
  pin_memory: true
  prefetch_factor: 4
  latent_cache_dir: data/latent_cache
  checkpoint_interval: 25
  amp: true
  amp_dtype: bfloat16
  compile: false
  compile_mode: default
  grad_clip: null
  ema_decay: 0.999
  lambda_inv_enc: 0.01
  lambda_inv_dec: 0.01
  use_inverse_losses: true
  inverse_loss_frequency: 1
  inverse_loss_warmup_epochs: 5
  inverse_loss_max_weight: 0.05
  query_sampling:
    enabled: true
    num_queries: 2048
    strategy: uniform
    curriculum:
      enabled: false
      start_queries: 4096
      end_queries: 1024
      warmup_epochs: 5
  physics_priors:
    enabled: true
    lambda_divergence: 0.0
    lambda_conservation: 0.05
    lambda_boundary: 0.05
    lambda_positivity: 0.0
    bc_value: 0.0
    bc_type: all
    lambda_latent_norm: 1.0e-4
    lambda_latent_diversity: 1.0e-4
  lambda_spectral: 0.05
  lambda_relative: 0.0
  log_per_task_metrics: true

stages:
  operator:
    epochs: 40
    optimizer:
      name: muon_hybrid
      lr: 1.4e-3
      weight_decay: 0.03
      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8
    scheduler:
      name: ReduceLROnPlateau
      mode: min
      factor: 0.5
      patience: 3
      min_lr: 1.0e-5
  diff_residual:
    epochs: 0
    patience: 5
    optimizer:
      name: muon_hybrid
      lr: 3.0e-4
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8
    scheduler:
      name: ReduceLROnPlateau
      mode: min
      factor: 0.5
      patience: 3
      min_lr: 1.0e-5
  consistency_distill:
    epochs: 0
    patience: 5
    optimizer:
      name: muon_hybrid
      lr: 3.0e-4
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8
    scheduler:
      name: CosineAnnealingLR
      t_max: 10
      eta_min: 1.0e-5

checkpoint:
  dir: checkpoints

evaluation:
  enabled: true
  split: test
  distributed: true

ttc:
  enabled: false

logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: eval-2task-baseline
    tags: [distributed, ddp, 2gpu, pdebench, multi-task, advection1d, darcy2d, upt, 128tokens, baseline]
    group: multi-task-ddp-eval
    notes: "Baseline eval (no TTC) on 2Ã—A100 SXM"
