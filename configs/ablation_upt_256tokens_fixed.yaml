# Ablation: 256 tokens (FIXED + MUON)
# Purpose: Fixed configuration for UPT Phase 2 with Muon optimizer
# Changes from original 256-token config:
#   - Optimizer: AdamW → Muon hybrid (2-3x faster convergence expected)
#   - Gradient clipping: disabled (Muon has bounded updates)
#   - Muon NS steps: 6 (increased for larger model stability)
#   - Inverse loss weights: 0.5 → 0.005 (100x reduction for larger model)
#   - Inverse loss frequency: 10 → 1 (every iteration)
#   - Warmup epochs: 15 → 10 (longer warmup for larger model)
#   - Total epochs: 25 → 50 (more time needed for larger model)
#   - Batch size: carefully tuned for memory
#   - Learning rate: scaled with sqrt(d_model)
#
# Note: This is a large model and may require an A100 or larger GPU

seed: 42
deterministic: false
benchmark: true

data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

latent:
  dim: 256
  tokens: 256

operator:
  pdet:
    input_dim: 256
    hidden_dim: 512
    depths: [3, 3, 3]
    group_size: 32  # Keep at 32 to avoid divisibility issues
    num_heads: 8    # 256/32 = 8 heads per group

diffusion:
  latent_dim: 256
  hidden_dim: 512

training:
  batch_size: 6   # Requires 80GB GPU
  time_stride: 2
  dt: 0.1
  patience: 5     # Stop early if no improvement (was 20, too high)

  num_workers: 8
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2

  checkpoint_interval: 20

  amp: true
  compile: true   # Critical for large model efficiency
  grad_clip: null  # CHANGED: Disabled (Muon has bounded updates)
  ema_decay: 0.999
  accum_steps: 6

  # UPT Inverse Losses (FIXED - gentler for large model)
  lambda_inv_enc: 0.005  # Even smaller for large model (100x reduction from 0.5)
  lambda_inv_dec: 0.005  # Even smaller for large model
  use_inverse_losses: true
  inverse_loss_frequency: 1  # Every iteration (was 10)
  inverse_loss_warmup_epochs: 10  # Longer warmup for stability
  inverse_loss_max_weight: 0.02  # Lower cap for large model

  lambda_spectral: 0.03  # Reduced for large model
  lambda_relative: 0.0

stages:
  operator:
    epochs: 50  # More epochs for large model convergence

    optimizer:
      name: muon_hybrid  # CHANGED: Use Muon+AdamW hybrid
      lr: 2.0e-3  # Scaled with sqrt(256/32), capped at 2e-3
      weight_decay: 0.05  # Higher weight decay for large model

      # Muon-specific parameters (for 2D+ matrix params)
      muon_momentum: 0.95    # Nesterov momentum
      muon_ns_steps: 6       # More iterations for larger model stability
      muon_backend: auto     # auto, flash, or torch

      # AdamW-specific parameters (for 1D vector params)
      betas: [0.9, 0.999]
      eps: 1.0e-8

  diff_residual:
    epochs: 15  # Increased from 3 to allow proper training
    patience: 5  # Early stopping if validation doesn't improve

    optimizer:
      name: muon_hybrid
      lr: 3.0e-4  # 6.7x lower than operator
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 6
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8

    scheduler:
      name: ReduceLROnPlateau
      mode: min
      factor: 0.5  # Halve LR when loss plateaus
      patience: 3  # Wait 3 epochs before reducing
      min_lr: 1.0e-5  # Don't go below this

  consistency_distill:
    epochs: 10  # Increased from 3 to allow proper training
    patience: 5  # Early stopping if validation doesn't improve

    optimizer:
      name: muon_hybrid
      lr: 3.0e-4  # Same as diffusion
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 6
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8

    scheduler:
      name: CosineAnnealingLR
      t_max: 10  # Match max epochs
      eta_min: 1.0e-5  # Minimum LR at end

checkpoint:
  dir: checkpoints

evaluation:
  enabled: true
  split: test

ttc:
  enabled: true
  debug: true
  steps: 1  # Oct 15 setting: minimal exploration for good baseline
  candidates: 6  # Oct 15 setting
  beam_width: 2  # Oct 15 setting
  horizon: 2
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 100  # Oct 15 setting

  sampler:
    tau_range: [0.2, 0.8]  # Oct 15 setting: narrow range
    noise_std: 0.01  # Oct 15 setting: minimal noise
    noise_schedule: [0.02, 0.01]  # Oct 15 setting

  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []

    weights:
      # Oct 15 settings: conservation penalties (validated on 64 tokens: +30.5%)
      mass: 1.0  # Oct 15 setting
      energy: 0.1  # Oct 15 setting
      penalty_negative: 0.5  # Oct 15 setting

    critic:
      weight: 0.0
      hidden_dim: 512
      dropout: 0.1

  decoder:
    latent_dim: 256
    query_dim: 2
    hidden_dim: 512
    mlp_hidden_dim: 512
    num_layers: 4  # Extra layer for expressiveness
    num_heads: 4
    frequencies: [1.0, 2.0, 4.0, 8.0]
    output_channels:
      rho: 1
      e: 1

logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers-ablation-256tokens-muon
    tags: [ablation, 256tokens, 256dim, phase2, fixed, inverse-losses-fixed, large-model, muon, muon-hybrid]
    group: upt-ablation-muon