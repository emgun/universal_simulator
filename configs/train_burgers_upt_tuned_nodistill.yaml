# Burgers1D Training with UPT Inverse Losses â€” TUNED VARIANT v3.0 (No Distillation)
# Based on analysis of run train-20251028_000918
# Changes:
# - Fixed gradient clipping for diffusion stage (max norm 17.24 detected)
# - Enhanced TTC hyperparameters (candidates 24, beam_width 8)
# - Operator: 35 epochs with grad_clip 0.3
# - Diffusion: 8 epochs with grad_clip 1.0
# - Distillation: DISABLED (epochs = 0)

seed: 42
deterministic: false
benchmark: true

data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

latent:
  dim: 16
  tokens: 32

operator:
  pdet:
    input_dim: 16
    hidden_dim: 96
    depths: [1, 1, 1]
    group_size: 12
    num_heads: 6

diffusion:
  latent_dim: 16
  hidden_dim: 96

training:
  batch_size: 12
  time_stride: 2
  dt: 0.1
  patience: 10

  num_workers: 4
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2

  latent_cache_dir: data/latent_cache

  checkpoint_interval: 50

  amp: true
  compile: true

  # TUNED: Aggressive gradient clipping for operator
  grad_clip: 0.3
  grad_clip_per_param: true

  ema_decay: 0.999
  accum_steps: 4

  # TUNED: Much smaller inverse loss weights
  lambda_inv_enc: 0.001
  lambda_inv_dec: 0.001

  # TUNED: Curriculum learning for inverse losses
  inverse_loss_warmup_epochs: 15
  inverse_loss_max_weight: 0.05

  use_inverse_losses: true
  inverse_loss_frequency: 10

  lambda_spectral: 0.05
  lambda_relative: 0.0

  distill_micro_batch: 3
  distill_num_taus: 5

  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

stages:
  operator:
    epochs: 35

    optimizer:
      name: adamw
      lr: 1.0e-3
      betas: [0.9, 0.999]
      weight_decay: 0.03

  diff_residual:
    epochs: 8
    # TUNED: Stronger gradient clipping for diffusion (max norm 17.24 detected)
    grad_clip: 1.0
    ema_decay: 0.999

    optimizer:
      name: adamw
      lr: 5.0e-5
      weight_decay: 0.015
      betas: [0.9, 0.999]

    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 3.0e-6

  consistency_distill:
    # DISABLED: Skip distillation stage for faster experimentation
    epochs: 0

  steady_prior:
    epochs: 0

ttc:
  enabled: true
  steps: 1
  # TUNED: Increase candidates for better exploration
  candidates: 24
  # TUNED: Increase beam width for better search
  beam_width: 8
  horizon: 1
  residual_threshold: 0.35
  gamma: 1.0
  # TUNED: Increase max evaluations to support larger beam search
  max_evaluations: 350

  sampler:
    tau_range: [0.15, 0.85]
    noise_std: 0.05
    noise_schedule: [0.08, 0.05, 0.02]

  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []

    weights:
      mass: 1.0
      energy: 1.0
      penalty_negative: 0.5

    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1

  decoder:
    latent_dim: 16
    query_dim: 2
    hidden_dim: 96
    mlp_hidden_dim: 128
    num_layers: 3
    num_heads: 4
    frequencies: [1.0, 2.0, 4.0, 8.0]

    output_channels:
      rho: 1
      e: 1

checkpoint:
  dir: checkpoints

evaluation:
  enabled: true
  split: test

logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers-upt-tuned-nodistill
    tags: [16dim, upt, inverse-losses, gradient-clipping, tuned-ttc, no-distill]
    group: upt-experiments

