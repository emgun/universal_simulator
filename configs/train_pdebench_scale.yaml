# Scale-ready training config for PDEBench Burgers1D (large runs).
include: defaults

data:
  task: burgers1d
  split: train
  root: data/pdebench  # Override with PDEBENCH_ROOT env var if needed
  patch_size: 1
  shard_limit: null

latent:
  dim: 512
  tokens: 128

training:
  batch_size: 4
  dt: 0.1
  patience: 4
  log_path: reports/pdebench_scale_training_log.jsonl
  amp: true
  grad_clip: 1.0
  ema_decay: 0.999
  accum_steps: 4
  num_workers: 8
  pin_memory: true
  prefetch_factor: 2
  latent_cache_dir: data/latent_cache
  latent_cache_dtype: float16

optimizer:
  name: adamw
  lr: 3.0e-4
  weight_decay: 0.01

stages:
  operator:
    epochs: 32
    optimizer:
      name: adamw
      lr: 2.5e-4
      weight_decay: 0.02
    scheduler:
      name: cosineannealinglr
      t_max: 40
      eta_min: 2.5e-5
  diff_residual:
    epochs: 20
    optimizer:
      name: adam
      lr: 7.5e-5
      weight_decay: 0.0
  consistency_distill:
    epochs: 12
    batch_size: 6  # reduced due to larger model footprint
    optimizer:
      name: adam
      lr: 4.0e-5
  steady_prior:
    epochs: 0
    optimizer:
      name: adam
      lr: 5.0e-5

checkpoint:
  dir: checkpoints/scale/

logging:
  wandb:
    enabled: true
    project: universal-simulator
    run_name: burgers1d-train-scale
    group: burgers1d-scale
    job_type: training
