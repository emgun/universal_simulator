# Scale-ready training config for PDEBench Burgers1D (large runs).
include: defaults

data:
  task: burgers1d
  split: train
  root: data/pdebench  # Override with PDEBENCH_ROOT env var if needed
  patch_size: 1
  shard_limit: null

latent:
  dim: 128
  tokens: 64

training:
  batch_size: 32
  dt: 0.1
  patience: 4
  log_path: reports/pdebench_scale_training_log.jsonl
  amp: true
  grad_clip: 1.0

optimizer:
  name: adamw
  lr: 3.0e-4
  weight_decay: 0.01

stages:
  operator:
    epochs: 30
    optimizer:
      name: adamw
      lr: 3.0e-4
      weight_decay: 0.01
    scheduler:
      name: cosineannealinglr
      t_max: 30
      eta_min: 3.0e-5
  diff_residual:
    epochs: 15
    optimizer:
      name: adam
      lr: 1.0e-4
      weight_decay: 0.0
  consistency_distill:
    epochs: 10
    optimizer:
      name: adam
      lr: 5.0e-5
  steady_prior:
    epochs: 20
    optimizer:
      name: adam
      lr: 5.0e-5

checkpoint:
  dir: checkpoints/scale/

logging:
  wandb:
    enabled: true
    project: universal-simulator
    run_name: burgers1d-train-scale
    group: burgers1d-scale
    job_type: training
