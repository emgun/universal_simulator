# Golden Burgers1D Training Configuration
# Based on successful run ptxr87mw (NRMSE: 0.0782 - 25x improvement)
# 
# Performance:
#   Baseline NRMSE: ~0.078 (vs 1.96 in previous configs)
#   TTC NRMSE: ~0.078 (minimal improvement, but stable)
#   Training time: ~14.5 min on RTX 4090
#   Architecture: 16-dim latent, 32 tokens, 96 hidden_dim
#
# Key Success Factors:
#   1. Dimensional consistency between training and TTC inference
#   2. More training epochs (25/8/8 vs 15/5/6)
#   3. Higher learning rates for diffusion/consistency stages
#   4. Better regularization with weight decay
#   5. Efficient token utilization (32 tokens, 16 latent)

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
seed: 42                    # Fixed random seed for reproducibility
deterministic: true         # Enable deterministic algorithms
benchmark: false            # Disable cudnn benchmarking for reproducibility

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  # No shard_limit - use full dataset

  download:
    test_val_datasets: burgers1d_full_v1
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

# ============================================================================
# LATENT SPACE
# ============================================================================
latent:
  dim: 16        # Key: Smaller latent dim for efficiency
  tokens: 32     # Key: More tokens for better spatial resolution

# ============================================================================
# OPERATOR ARCHITECTURE
# ============================================================================
operator:
  pdet:
    input_dim: 16           # Must match latent.dim
    hidden_dim: 96          # 6x latent_dim (enhanced capacity)
    depths: [1, 1, 1]
    group_size: 12          # Divides hidden_dim evenly (96/12=8)
    num_heads: 6            # Enhanced attention heads

# ============================================================================
# DIFFUSION ARCHITECTURE
# ============================================================================
diffusion:
  latent_dim: 16            # Must match latent.dim
  hidden_dim: 96            # Match operator hidden_dim

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  batch_size: 12
  time_stride: 2
  dt: 0.1
  patience: 10

  num_workers: 8
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2

  latent_cache_dir: data/latent_cache
  latent_cache_dtype: float32
  checkpoint_interval: 50

  amp: true
  compile: true
  grad_clip: 1.0
  ema_decay: 0.999
  accum_steps: 4

  distill_micro_batch: 3
  distill_num_taus: 5

  lambda_spectral: 0.05
  lambda_relative: 0.0

  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

# ============================================================================
# TRAINING STAGES
# ============================================================================
stages:
  operator:
    epochs: 25              # Key: More epochs (was 15)
    
    optimizer:
      name: adamw
      lr: 1.0e-3
      betas: [0.9, 0.999]
      weight_decay: 0.03    # Key: Added weight decay


  diff_residual:
    epochs: 8               # Key: More epochs (was 5)
    grad_clip: 1.0
    ema_decay: 0.999

    optimizer:
      name: adamw
      lr: 5.0e-5            # Key: Higher LR (was 3e-5)
      weight_decay: 0.015   # Key: Added weight decay
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 3.0e-6

  consistency_distill:
    epochs: 8               # Key: More epochs (was 6)
    batch_size: 6
    tau_schedule: [5, 4, 3]
    accum_steps: 2

    optimizer:
      name: adamw
      lr: 3.0e-5            # Key: Higher LR (was 2e-5)
      weight_decay: 0.015   # Key: Added weight decay
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 2.0e-6

  steady_prior:
    epochs: 0

# ============================================================================
# TEST-TIME CONDITIONING (TTC)
# ============================================================================
ttc:
  enabled: true
  steps: 1
  candidates: 16              # Increased from 8 for better search
  beam_width: 5               # Increased from 3 for wider exploration
  horizon: 1
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 200        # Increased from 150 to allow more exploration

  sampler:
    tau_range: [0.15, 0.85]
    noise_std: 0.05           # Increased from 0.015 for more diversity
    noise_schedule: [0.08, 0.05, 0.02]  # Updated schedule

  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []

    weights:
      mass: 1.0               # Reduced from 1.2 for balance
      energy: 1.0             # Increased from 0.15 (KEY FIX)
      penalty_negative: 0.5   # Reduced from 0.6 for balance

    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1

  decoder:
    latent_dim: 16          # Key: Matches main latent dim
    query_dim: 2
    hidden_dim: 96          # Key: Matches main hidden dim
    mlp_hidden_dim: 128
    num_layers: 3
    num_heads: 4
    frequencies: [1.0, 2.0, 4.0, 8.0]

    output_channels:
      rho: 1
      e: 1

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================
checkpoint:
  dir: checkpoints

# ============================================================================
# EVALUATION
# ============================================================================
evaluation:
  enabled: true
  split: test

# ============================================================================
# LOGGING
# ============================================================================
logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers-golden
    tags: [16dim, golden, practical, efficient]
    group: baseline-experiments
