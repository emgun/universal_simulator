# Training configuration for multi-PDE latent operator stages.
include: defaults

optimizer:
  name: adam
  lr: 1.0e-3
  weight_decay: 1.0e-5

stages:
  operator:
    epochs: 2
  diff_residual:
    epochs: 2
  consistency_distill:
    epochs: 1
  steady_prior:
    epochs: 2

checkpoint:
  dir: checkpoints/

training:
  batch_size: 16
  dt: 0.1
  patience: 1
  log_path: reports/training_log.jsonl
