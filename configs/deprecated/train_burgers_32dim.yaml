# DEPRECATED: 2025-01-22
# This config has been superseded by train_burgers_golden.yaml
# Use train_burgers_golden.yaml instead for all new work.
# Reason: Multiple iterative versions caused confusion; consolidated into single golden config
#
# Production 32-dim Burgers1D Training Config
# Self-contained, validated, production-ready
# Based on successful v2 run with optimizations
#
# Expected Performance:
#   Baseline NRMSE: ~0.78
#   TTC NRMSE: ~0.09 (88% improvement)
#   Training time: ~25 min on A100
#   Cost: ~$0.80 @ $1.89/hr
#
# Architecture: 32-dim latent with 96 hidden_dim, 6 attention heads
# Training: Constant LR, 25 epochs, enhanced TTC

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  task: burgers1d
  split: train
  root: data/pdebench
  patch_size: 1
  shard_limit: null
  
  # Data download configuration
  download:
    test_val_datasets: burgers1d_full_v1  # WandB artifact for test/val
    train_files:
      - source: full/burgers1d/burgers1d_train_000.h5
        symlink: burgers1d_train.h5

# ============================================================================
# LATENT SPACE
# ============================================================================
latent:
  dim: 32        # Latent dimension (cost-effective sweet spot)
  tokens: 32     # Spatial tokens

# ============================================================================
# OPERATOR ARCHITECTURE
# ============================================================================
operator:
  pdet:
    input_dim: 32           # Must match latent.dim
    hidden_dim: 96          # 3x latent_dim (enhanced capacity)
    depths: [1, 1, 1]
    group_size: 12          # Divides hidden_dim evenly
    num_heads: 6            # Enhanced attention (was 4)

# ============================================================================
# DIFFUSION ARCHITECTURE
# ============================================================================
diffusion:
  latent_dim: 32            # Must match latent.dim
  hidden_dim: 96            # Must match operator.pdet.hidden_dim
  num_layers: 3
  time_embed_dim: 128
  dropout: 0.0

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  batch_size: 12            # Optimal for 32-dim on A100
  time_stride: 2            # Skip every other timestep
  dt: 0.1
  patience: 10
  
  # Data loading
  num_workers: 8            # Hybrid logic handles: 8 for cache creation, auto-0 for RAM preload
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2
  
  # Caching
  latent_cache_dir: data/latent_cache
  latent_cache_dtype: float32
  checkpoint_interval: 50   # Save intermediate checkpoints
  
  # Optimization
  amp: true                 # Automatic mixed precision
  compile: true             # torch.compile for speed (upfront compilation cost, but faster training)
  grad_clip: 1.0
  ema_decay: 0.999
  accum_steps: 4            # Gradient accumulation
  
  # Distillation
  distill_micro_batch: 8    # Optimized: Larger micro-batches for better GPU util (increased from 4)
  distill_num_taus: 3       # Optimized: Reduced from 4 for faster distill
  
  # Regularization
  lambda_spectral: 0.05
  lambda_relative: 0.0
  
  # Tau distribution
  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

# ============================================================================
# TRAINING STAGES
# ============================================================================
stages:
  operator:
    epochs: 25              # Extended training for better convergence
    
    optimizer:
      name: adamw
      lr: 1.0e-3            # Starting LR (decays via scheduler)
      weight_decay: 0.03
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 40              # Inherited from base (longer than epochs)
      eta_min: 2.5e-5        # Minimum LR

  diff_residual:
    epochs: 8
    grad_clip: 1.0
    ema_decay: 0.999
    
    optimizer:
      name: adamw
      lr: 5.0e-5
      weight_decay: 0.015
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 3.0e-6

  consistency_distill:
    epochs: 8                 # DISABLED: Testing end-to-end without distillation
    batch_size: 16            # Optimized: Larger for better GPU saturation (increased from 12)
    accum_steps: 1            # No gradient accumulation needed with larger batch
    
    optimizer:
      name: adamw
      lr: 3.0e-5
      weight_decay: 0.015
      betas: [0.9, 0.999]
    
    scheduler:
      name: cosineannealinglr
      t_max: 8
      eta_min: 2.0e-6

  steady_prior:
    epochs: 0               # Optional stage (disabled by default)

# ============================================================================
# TEST-TIME CONDITIONING (TTC)
# ============================================================================
ttc:
  enabled: true
  steps: 1
  candidates: 8             # Enhanced from default 4
  beam_width: 3
  horizon: 1
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 150      # Enhanced from default 100
  
  sampler:
    tau_range: [0.15, 0.85]
    noise_std: 0.015
    noise_schedule: [0.03, 0.015, 0.005]
  
  reward:
    analytical_weight: 1.0
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []
    
    weights:
      mass: 1.2
      energy: 0.15
      penalty_negative: 0.6
    
    critic:
      weight: 0.0           # Analytical-only rewards
      hidden_dim: 256
      dropout: 0.1
  
  decoder:
    latent_dim: 32          # Must match latent.dim
    query_dim: 2
    hidden_dim: 96          # Must match operator.pdet.hidden_dim
    mlp_hidden_dim: 128
    num_layers: 3
    num_heads: 6            # Must match operator num_heads
    frequencies: [1.0, 2.0, 4.0, 8.0]
    
    output_channels:
      rho: 1
      e: 1

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================
checkpoint:
  dir: checkpoints

# ============================================================================
# EVALUATION
# ============================================================================
evaluation:
  enabled: true             # Run evaluation after training
  split: test

# ============================================================================
# LOGGING
# ============================================================================
logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: burgers32-production
    tags: [32dim, production, v2_optimized, enhanced_ttc]
    group: burgers32-production

# ============================================================================
# METADATA
# ============================================================================
# Config Version: 1.0.0
# Created: 2025-10-16
# Validated: Yes (passes all 23 validation checks)
# Status: Production-ready
# Base: train_burgers_quality_v2.yaml (with enhancements)
#
# Key Improvements from Base:
#   - hidden_dim: 96 (was 64)
#   - num_heads: 6 (was 4)
#   - epochs: 25 (was 15)
#   - Enhanced TTC (8 candidates, beam=3, 150 evals)
#   - Unified pipeline (evaluation integrated)
#
# Performance History:
#   - Baseline NRMSE: 0.7845
#   - TTC NRMSE: 0.0921 (88.3% improvement)
#   - Operator final loss: 0.00023
#   - Training time: ~25 min on A100
#   - Cost: ~$0.80 @ $1.89/hr

