# 11-Task DDP (192d): All PDEBench tasks with Distributed Data Parallel (4×GPU)
# Purpose: Full-scale multi-task distributed training across entire PDEBench suite
#
# Key Differences from 2-Task Baseline:
#   - Distributed: num_gpus: 4 (4×A100 = 320GB total)
#   - Tasks: All 11 PDEBench tasks (vs 2 tasks)
#   - Latent dim: 192 (vs 128, more capacity for 11 tasks)
#   - Hidden dim: 576 (vs 384, 3× latent)
#   - Depth: 16 (vs 12, deeper for more tasks)
#   - Per-GPU batch size: 12 (vs 8)
#   - Gradient accumulation: 4 steps (vs 6)
#   - Effective batch: 12*4*4 = 192 (vs 96)

seed: 42
deterministic: false
benchmark: true

data:
  # All 11 PDEBench tasks
  task:
    - advection1d        # 1D advection equation
    - darcy2d            # 2D Darcy flow
    - burgers1d          # 1D Burgers equation
    - diff_react1d       # 1D diffusion-reaction
    - swe2d              # 2D shallow water equations
    - ns2d_cond          # 2D Navier-Stokes (incompressible, conditional)
    - comp_ns2d          # 2D compressible Navier-Stokes
    - react_diff2d       # 2D reaction-diffusion
    - diff_sorp2d        # 2D diffusion-sorption
    - shallow_water2d_varied  # 2D shallow water (varied bathymetry)
    - ns2d_turb          # 2D Navier-Stokes (turbulent)

  split: train
  root: data/pdebench
  patch_size: 1

  # Task sampling strategy (critical for 11 tasks)
  task_sampling:
    strategy: "balanced"  # Equal samples per task per epoch (MultiTaskDistributedSampler)

latent:
  dim: 192          # Increased from 128 for 11-task capacity
  tokens: 192       # Increased from 128

operator:
  # CRITICAL: Use Phase 3's pure transformer architecture
  architecture_type: pdet_stack    # Pure transformer (NOT pdet_unet)

  pdet:
    input_dim: 192                   # Must match latent.dim
    hidden_dim: 576                  # 3× latent.dim (increased from 384)
    depth: 16                        # Deeper for 11 tasks (increased from 12)
    num_heads: 8
    attention_type: standard         # CRITICAL: Use standard attention (NOT channel_separated)
    qk_norm: true                    # QK normalization for stability
    mlp_ratio: 4.0                   # Standard transformer expansion
    drop_path: 0.1                   # Stochastic depth for regularization
    dropout: 0.0

diffusion:
  latent_dim: 192                    # Must match latent.dim
  hidden_dim: 576                    # Match operator

training:
  # Distributed training (4×GPU)
  num_gpus: 4                        # 4×A100 = 320GB total

  # Memory optimization for 11 tasks on 4 GPUs
  batch_size: 12                     # Per-GPU (12*4 = 48 samples per step)
  accum_steps: 4                     # Effective batch = 12*4*4 = 192

  time_stride: 2
  dt: 0.1
  patience: 3

  # Parallel encoding (enabled for multi-GPU)
  num_workers: 8                     # 8 workers per GPU
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 4
  cache_dir: data/latent_cache       # CRITICAL: Use precomputed cache for 4-8x speedup

  checkpoint_interval: 25

  amp: true
  compile: false
  compile_mode: reduce-overhead
  grad_clip: null
  ema_decay: 0.999

  # UPT Inverse Losses
  lambda_inv_enc: 0.01
  lambda_inv_dec: 0.01
  use_inverse_losses: true
  inverse_loss_frequency: 1
  inverse_loss_warmup_epochs: 5
  inverse_loss_max_weight: 0.05

  # Query-Based Training
  query_sampling:
    enabled: true
    num_queries: 2048                # Sample 2k points per batch
    strategy: uniform

    curriculum:
      enabled: false

  # Physics Priors + Latent Regularization
  physics_priors:
    enabled: true

    # Conservative settings for diverse 11-task suite
    lambda_divergence: 0.0
    lambda_conservation: 0.0         # DISABLED: Multi-task tensor size mismatch
    lambda_boundary: 0.05
    lambda_positivity: 0.0
    bc_value: 0.0
    bc_type: all

    # Latent Regularization (important for 11 tasks)
    lambda_latent_norm: 1.0e-4       # Prevent latent collapse/explosion
    lambda_latent_diversity: 1.0e-4  # Prevent token collapse

  lambda_spectral: 0.05
  lambda_relative: 0.0

  # Per-task metric logging (critical for 11 tasks)
  log_per_task_metrics: true

stages:
  operator:
    epochs: 50                       # More epochs for 11 tasks (vs 40 for 2 tasks)

    optimizer:
      name: muon_hybrid
      lr: 1.2e-3                     # Slightly reduced for stability with more tasks
      weight_decay: 0.03

      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto

      betas: [0.9, 0.999]
      eps: 1.0e-8

  diff_residual:
    epochs: 0
    patience: 3

    optimizer:
      name: muon_hybrid
      lr: 3.0e-4
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8

    scheduler:
      name: ReduceLROnPlateau
      mode: min
      factor: 0.5
      patience: 3
      min_lr: 1.0e-5

  consistency_distill:
    epochs: 0
    patience: 3

    optimizer:
      name: muon_hybrid
      lr: 3.0e-4
      weight_decay: 0.01
      muon_momentum: 0.95
      muon_ns_steps: 5
      muon_backend: auto
      betas: [0.9, 0.999]
      eps: 1.0e-8

    scheduler:
      name: CosineAnnealingLR
      t_max: 10
      eta_min: 1.0e-5

checkpoint:
  dir: checkpoints

evaluation:
  enabled: true
  split: test

# Test-Time Conditioning (TTC) - Adapted for 11-task PDEBench
ttc:
  enabled: true
  debug: true
  steps: 1
  candidates: 8                      # More candidates for diverse tasks
  beam_width: 3                      # Wider beam for exploration
  horizon: 2
  residual_threshold: 0.35
  gamma: 1.0
  max_evaluations: 150               # More evaluations for 11 tasks

  sampler:
    tau_range: [0.2, 0.8]
    noise_std: 0.01
    noise_schedule: [0.02, 0.01]

  reward:
    analytical_weight: 1.0
    grid: [64, 64]

    # Generic field names for multi-task
    mass_field: "u"                  # Generic quantity field
    energy_field: null
    momentum_field: []

    weights:
      mass: 1.0                      # PRIMARY: Mass/quantity conservation
      energy: 0.0
      penalty_negative: 0.0

    critic:
      weight: 0.0                    # Use analytical rewards only
      hidden_dim: 256
      dropout: 0.1

  decoder:
    latent_dim: 192                  # Match training latent dim
    query_dim: 2                     # 2D max
    hidden_dim: 576                  # Match operator.pdet.hidden_dim
    mlp_hidden_dim: 576              # 3× latent_dim
    num_layers: 3
    num_heads: 8
    frequencies: [1.0, 2.0, 4.0, 8.0]
    output_channels:
      u: 1                           # Generic field name for PDEBench

logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: pdebench-11task-ddp-192d
    tags:
      - distributed
      - ddp
      - 4gpu
      - pdebench
      - multi-task
      - 11task
      - 192d
      - production
    group: distributed-training-11task
    notes: "Distributed Data Parallel training for full 11-task PDEBench suite on 4×A100 GPUs. Scaled architecture: 192d latent, 576d hidden, depth 16."
