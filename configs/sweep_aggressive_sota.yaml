# Aggressive SOTA Push
# Combines: Higher capacity + optimal optimizer + extended training
# Target: <0.035 nRMSE

include: train_burgers_32dim.yaml

# ============================================================================
# LATENT SPACE - LARGE CAPACITY
# ============================================================================
latent:
  dim: 48  # Increased from 32
  tokens: 48  # Significant increase

# ============================================================================
# OPERATOR ARCHITECTURE - LARGE MODEL
# ============================================================================
operator:
  pdet:
    input_dim: 48
    hidden_dim: 128  # Significantly larger
    depths: [2, 2, 2]  # Deeper
    group_size: 16
    num_heads: 8

# ============================================================================
# DIFFUSION ARCHITECTURE - LARGE
# ============================================================================
diffusion:
  latent_dim: 48
  hidden_dim: 128

# ============================================================================
# TRAINING CONFIGURATION - OPTIMIZED
# ============================================================================
training:
  batch_size: 8  # Reduced for large model
  time_stride: 2
  dt: 0.1
  patience: 15  # More patience for large model
  num_workers: 8
  use_parallel_encoding: true
  pin_memory: true
  prefetch_factor: 2
  latent_cache_dir: data/latent_cache
  latent_cache_dtype: float32
  checkpoint_interval: 50
  amp: true
  compile: true
  grad_clip: 0.8
  ema_decay: 0.9997  # Higher for stability
  accum_steps: 6  # Maintain effective batch size

  distill_micro_batch: 2
  distill_num_taus: 8  # More sampling points

  lambda_spectral: 0.08  # Increased for better spectral accuracy
  lambda_relative: 0.0

  tau_distribution:
    type: beta
    alpha: 1.2
    beta: 1.2

# ============================================================================
# TRAINING STAGES - EXTENDED & OPTIMIZED
# ============================================================================
stages:
  operator:
    epochs: 30  # Extended training

    optimizer:
      name: adamw
      lr: 3.5e-4  # Scaled for model size
      betas: [0.9, 0.95]
      weight_decay: 0.03  # Regularization

    scheduler:
      name: cosineannealinglr
      t_max: 30
      eta_min: 1.0e-5
      warmup_steps_ratio: 0.06

  diff_residual:
    epochs: 12  # Extended

    optimizer:
      name: adamw
      lr: 3.5e-5
      weight_decay: 0.02
      betas: [0.9, 0.95]

    scheduler:
      name: cosineannealinglr
      t_max: 12
      eta_min: 3.5e-6

  consistency_distill:
    epochs: 12  # Extended
    batch_size: 4  # Small for large model
    accum_steps: 3

    optimizer:
      name: adamw
      lr: 2.5e-5
      weight_decay: 0.02
      betas: [0.9, 0.95]

    scheduler:
      name: cosineannealinglr
      t_max: 12
      eta_min: 2.5e-6

  steady_prior:
    epochs: 0

# ============================================================================
# TTC - ENHANCED
# ============================================================================
ttc:
  enabled: true
  steps: 2  # More refinement steps
  candidates: 12  # More exploration
  beam_width: 4  # Wider beam
  horizon: 1
  residual_threshold: 0.3  # Tighter threshold
  gamma: 1.2  # Stronger physics weighting
  max_evaluations: 200

  sampler:
    tau_range: [0.1, 0.9]  # Wider range
    noise_std: 0.012  # Slightly lower noise
    noise_schedule: [0.025, 0.012, 0.004]

  reward:
    analytical_weight: 1.2  # Stronger analytical weight
    grid: [64, 64]
    mass_field: rho
    energy_field: e
    momentum_field: []

    weights:
      mass: 1.5  # Stronger conservation
      energy: 0.2
      penalty_negative: 0.8

    critic:
      weight: 0.0
      hidden_dim: 256
      dropout: 0.1

  decoder:
    latent_dim: 48
    query_dim: 2
    hidden_dim: 128
    mlp_hidden_dim: 256
    num_layers: 4
    num_heads: 8
    frequencies: [1.0, 2.0, 4.0, 8.0, 16.0]  # More frequencies

    output_channels:
      rho: 1
      e: 1

# ============================================================================
# LOGGING
# ============================================================================
logging:
  wandb:
    enabled: true
    project: universal-simulator
    entity: emgun-morpheus-space
    run_name: sweep-aggressive-sota
    tags: [aggressive-sota, dim-48, tokens-48, hidden-128, extended-training]
    group: sota-aggressive
